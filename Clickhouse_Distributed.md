**Создание шардированных таблиц в ClickHouse с использованием движка Distributed**

---

### **Введение**

ClickHouse — это высокопроизводительная колоночная СУБД, которая поддерживает горизонтальное масштабирование через шардирование и репликацию. Для реализации шардирования используется движок **Distributed**, который распределяет запросы и данные по указанным шардам.

---

### **Шаг 1: Настройка кластера**

Перед созданием шардированных таблиц необходимо настроить конфигурацию кластера. Это делается в файле конфигурации `clusters.xml`, который обычно находится в `/etc/clickhouse-server/config.d/` или может быть добавлен в основной конфигурационный файл `config.xml`.

**Пример конфигурации кластера:**

```xml
<clickhouse>
    <remote_servers>
        <my_cluster>
            <shard>
                <replica>
                    <host>host1</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>host2</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <replica>
                    <host>host3</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>host4</host>
                    <port>9000</port>
                </replica>
            </shard>
        </my_cluster>
    </remote_servers>
</clickhouse>
```

В этом примере кластер `my_cluster` состоит из двух шардов, каждый из которых имеет две реплики.

**Важно:** После изменения конфигурации сервера необходимо перезапустить ClickHouse Server.

---

### **Шаг 2: Создание локальных таблиц на каждом шарде**

На каждом сервере создайте локальную таблицу, используя движок **MergeTree** или его вариации.

**Пример создания локальной таблицы:**

```sql
CREATE TABLE default.local_table ON CLUSTER my_cluster
(
    event_date Date,
    event_time DateTime,
    user_id UInt32,
    event_type String,
    event_value Float32
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(event_date)
ORDER BY (event_date, user_id);
```

**Объяснение:**

- **ON CLUSTER my_cluster**: Эта команда создаст таблицу на всех узлах кластера `my_cluster`.
- **ENGINE = MergeTree()**: Используется движок MergeTree, оптимизированный для больших объемов данных.
- **PARTITION BY**: Указывает, как будут разбиваться данные на партиции.
- **ORDER BY**: Определяет порядок сортировки данных внутри партиции.

---

### **Шаг 3: Создание Distributed таблицы**

Distributed таблица является логической таблицей, которая объединяет данные из локальных таблиц на разных шардах и репликах.

**Пример создания Distributed таблицы:**

```sql
CREATE TABLE default.distributed_table ON CLUSTER my_cluster AS default.local_table
ENGINE = Distributed(
    'my_cluster',     -- имя кластера из конфигурации
    'default',        -- база данных локальных таблиц
    'local_table',    -- имя локальной таблицы
    user_id           -- ключ шардирования
);
```

**Объяснение:**

- **AS default.local_table**: Структура таблицы совпадает с локальной таблицей `local_table`.
- **ENGINE = Distributed(...)**: Указывает, что это Distributed таблица.
- **'my_cluster'**: Имя кластера из конфигурации.
- **'default'**: База данных, в которой находятся локальные таблицы.
- **'local_table'**: Имя локальной таблицы.
- **user_id**: Ключ шардирования; определяет, на какой шард попадет строка.

---

### **Шаг 4: Вставка данных**

**Вариант 1: Вставка через Distributed таблицу**

Вы можете вставлять данные напрямую в Distributed таблицу — ClickHouse автоматически распределит данные по шардам.

**Пример:**

```sql
INSERT INTO distributed_table (event_date, event_time, user_id, event_type, event_value) VALUES
('2023-01-01', '2023-01-01 12:00:00', 123, 'click', 1.0),
('2023-01-02', '2023-01-02 13:00:00', 456, 'view', 2.0);
```

**Вариант 2: Вставка в локальные таблицы**

Можно вставлять данные непосредственно в локальные таблицы на конкретных узлах. Это полезно для загрузки данных в оффлайн-режиме или при миграции данных.

---

### **Шаг 5: Запросы к данным**

**Запрос через Distributed таблицу**

Вы можете выполнять запросы к Distributed таблице, и ClickHouse сам распределит запрос по шардам и объединит результаты.

**Пример:**

```sql
SELECT
    event_type,
    COUNT(*) AS event_count,
    AVG(event_value) AS average_value
FROM
    distributed_table
WHERE
    event_date >= '2023-01-01' AND event_date <= '2023-01-31'
GROUP BY
    event_type;
```

---

### **Полный пример**

**1. Создание локальной таблицы на всех узлах:**

```sql
CREATE TABLE default.local_table ON CLUSTER my_cluster
(
    event_date Date,
    event_time DateTime,
    user_id UInt32,
    event_type String,
    event_value Float32
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(event_date)
ORDER BY (event_date, user_id);
```

**2. Создание Distributed таблицы:**

```sql
CREATE TABLE default.distributed_table ON CLUSTER my_cluster AS default.local_table
ENGINE = Distributed(
    'my_cluster',
    'default',
    'local_table',
    user_id
);
```

**3. Вставка данных:**

```sql
INSERT INTO distributed_table (event_date, event_time, user_id, event_type, event_value) VALUES
('2023-01-01', '2023-01-01 12:00:00', 123, 'click', 1.0),
('2023-01-02', '2023-01-02 13:00:00', 456, 'view', 2.0);
```

**4. Запрос данных:**

```sql
SELECT
    event_type,
    COUNT(*) AS event_count,
    AVG(event_value) AS average_value
FROM
    distributed_table
WHERE
    event_date BETWEEN '2023-01-01' AND '2023-01-31'
GROUP BY
    event_type;
```

---

### **Дополнительные детали**

#### **Шардирование**

Ключ шардирования (`user_id` в нашем примере) определяет, на какой шард попадут данные. Выбор правильного ключа шардирования важен для:

- **Равномерного распределения данных** по шардам.
- **Минимизации перекрестных запросов** между шардами.

#### **Репликация**

Если у вас настроена репликация, ClickHouse обеспечивает синхронизацию данных между репликами в рамках одного шарда. Это повышает отказоустойчивость и обеспечивает доступность данных.

#### **Параметры конфигурации**

- **insert_distributed_sync**: Если установить в `1`, вставки в Distributed таблицу будут синхронными, что гарантирует запись данных на все шарды до завершения операции.
- **insert_distributed_timeout**: Устанавливает таймаут для синхронных вставок.
- **prefer_localhost_replica**: Если включено, запросы будут предпочитать локальную реплику для минимизации сетевой нагрузки.

---

### **Практические советы**

- **Убедитесь, что все узлы имеют одинаковую конфигурацию таблиц**, чтобы избежать ошибок при выполнении запросов.
- **Выбирайте ключ шардирования с осторожностью**, основываясь на характеристиках ваших данных и запросов.
- **Используйте функционал DDL-запросов на кластере**, чтобы автоматически создавать таблицы на всех узлах:

```sql
CREATE TABLE database_name.table_name ON CLUSTER my_cluster
(
    -- описание столбцов
)
ENGINE = ...;
```

- **Мониторьте состояние кластера** с помощью системных таблиц:

```sql
SELECT * FROM system.clusters WHERE cluster = 'my_cluster';
SELECT * FROM system.tables WHERE name = 'local_table';
```

- **При необходимости используйте секционирование данных (partitioning)** для улучшения производительности при работе с историческими данными.

---

### **Пример с использованием Python и ClickHouse-driver**

Если вы хотите вставлять данные программно, можно использовать официальный Python-клиент `clickhouse-driver`.

**Установка библиотеки:**

```bash
pip install clickhouse-driver
```

**Пример кода:**

```python
from clickhouse_driver import Client

client = Client('host1')  # Подключение к одному из узлов кластера

# Вставка данных в Distributed таблицу
data = [
    ('2023-01-01', '2023-01-01 12:00:00', 123, 'click', 1.0),
    ('2023-01-02', '2023-01-02 13:00:00', 456, 'view', 2.0),
]

client.execute(
    'INSERT INTO default.distributed_table (event_date, event_time, user_id, event_type, event_value) VALUES',
    data
)

# Выполнение запроса
result = client.execute('''
    SELECT
        event_type,
        COUNT(*) AS event_count,
        AVG(event_value) AS average_value
    FROM
        default.distributed_table
    WHERE
        event_date BETWEEN '2023-01-01' AND '2023-01-31'
    GROUP BY
        event_type
''')

print(result)
```

---

### **Заключение**

Использование шардирования и движка **Distributed** в ClickHouse позволяет эффективно масштабировать систему для обработки больших объемов данных. Создание локальных таблиц на шардах и объединение их с помощью Distributed таблицы предоставляет прозрачный механизм для работы с распределенными данными.

**Ключевые моменты:**

- **Настройка кластера** в конфигурационных файлах ClickHouse.
- **Создание локальных таблиц** на каждом шарде с одинаковой структурой.
- **Создание Distributed таблицы**, которая объединяет локальные таблицы и обеспечивает прозрачный доступ к данным.
- **Вставка и запрос данных** через Distributed таблицу так же, как и с обычной таблицей.

---

**Дополнительные ресурсы:**

- [Документация ClickHouse по Distributed таблицам](https://clickhouse.com/docs/ru/engines/table-engines/special/distributed)
- [Руководство по настройке кластеров в ClickHouse](https://clickhouse.com/docs/ru/operations/cluster-setup)
- [Практики шардирования и репликации](https://clickhouse.com/docs/ru/architecture/capabilities)

---

# Создание таблиц с шардированием на одном хосте ClickHouse: Пример и Разъяснение

---

### **Введение**

**Шардирование** — это процесс разделения данных на части (шарды) для распределения нагрузки и хранения больших объемов данных на нескольких серверах. Однако иногда возникает необходимость настроить шардирование на одном хосте. Это может быть полезно для тестирования, разработки или симуляции кластерной среды без дополнительных затрат на инфраструктуру.

---

### **Пример создания таблиц с шардированием на одном хосте**

#### **Шаг 1: Настройка кластера в конфигурации**

Даже на одном хосте необходимо настроить кластерную конфигурацию для имитации нескольких шардов. Это делается путем определения нескольких шардов и реплик, которые указывают на один и тот же хост, но используют разные порты или базы данных.

**Создание файла конфигурации `clusters.xml`** (если его нет):

Файл может быть размещен в директории `/etc/clickhouse-server/config.d/` или может быть добавлен в основной конфигурационный файл `config.xml`.

**Пример `clusters.xml`:**

```xml
<clickhouse>
    <remote_servers>
        <my_single_host_cluster>
            <shard>
                <replica>
                    <host>localhost</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <replica>
                    <host>localhost</host>
                    <port>9000</port>
                </replica>
            </shard>
        </my_single_host_cluster>
    </remote_servers>
</clickhouse>
```

**Объяснение:**

- **<my_single_host_cluster>** — имя нашего кластера.
- Мы определяем **два шарда**, каждый из которых ссылается на `localhost` и порт `9000`. Несмотря на то, что они указывают на один и тот же хост и порт, ClickHouse будет рассматривать их как отдельные шарды.

**Важно:** После изменения конфигурации перезапустите ClickHouse Server:

```bash
sudo systemctl restart clickhouse-server
```

#### **Шаг 2: Создание локальных таблиц для каждого шарда**

Создадим две локальные таблицы, каждая из которых соответствует своему шару. Мы можем использовать разные базы данных для имитации разных шардов.

**Создание баз данных для шардов:**

```sql
CREATE DATABASE shard1;
CREATE DATABASE shard2;
```

**Создание локальных таблиц в каждой базе данных:**

**В базе данных `shard1`:**

```sql
CREATE TABLE shard1.events_local
(
    event_date Date,
    event_time DateTime,
    user_id UInt32,
    event_type String,
    event_value Float32
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(event_date)
ORDER BY (event_date, user_id);
```

**В базе данных `shard2`:**

```sql
CREATE TABLE shard2.events_local
(
    event_date Date,
    event_time DateTime,
    user_id UInt32,
    event_type String,
    event_value Float32
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(event_date)
ORDER BY (event_date, user_id);
```

#### **Шаг 3: Создание Distributed таблицы**

Создадим Distributed таблицу, которая объединит данные из обоих локальных таблиц.

**В базе данных `default`:**

```sql
CREATE TABLE default.events
(
    event_date Date,
    event_time DateTime,
    user_id UInt32,
    event_type String,
    event_value Float32
)
ENGINE = Distributed(
    'my_single_host_cluster',  -- имя кластера
    '{database}',              -- подстановка имени базы данных шарда
    'events_local',            -- имя локальной таблицы в шардах
    user_id                    -- ключ шардирования
);
```

**Объяснение:**

- **'my_single_host_cluster'**: имя кластера из конфигурации.
- **'{database}'**: используется подстановка имени базы данных для соответствия базам `shard1` и `shard2`.
- **'events_local'**: имя локальной таблицы в каждой базе данных.
- **user_id**: ключ шардирования, который определяет, в какой шард попадут данные.

#### **Шаг 4: Вставка данных**

При вставке данных в Distributed таблицу ClickHouse будет распределять их по локальным таблицам на основе ключа шардирования.

**Пример вставки данных:**

```sql
INSERT INTO default.events (event_date, event_time, user_id, event_type, event_value) VALUES
('2023-01-01', '2023-01-01 12:00:00', 101, 'click', 1.0),
('2023-01-02', '2023-01-02 13:00:00', 202, 'view', 2.0),
('2023-01-03', '2023-01-03 14:00:00', 303, 'purchase', 3.0);
```

**Проверка, куда попали данные:**

**В базе данных `shard1`:**

```sql
SELECT * FROM shard1.events_local;
```

**В базе данных `shard2`:**

```sql
SELECT * FROM shard2.events_local;
```

Вы увидите, что данные распределились по шардам на основе значения `user_id`.

#### **Шаг 5: Запросы к данным**

**Запрос через Distributed таблицу:**

```sql
SELECT
    event_type,
    COUNT(*) AS event_count,
    AVG(event_value) AS average_value
FROM
    default.events
GROUP BY
    event_type;
```

ClickHouse автоматически выполнит запросы к локальным таблицам и объединит результаты.

---

### **Правильное ли это решение?**

**Преимущества настройки шардирования на одном хосте:**

1. **Тестирование и разработка:**
   - Позволяет разработчикам отладить и протестировать логику шардирования и распределенных запросов без необходимости в многосерверной среде.
   - Удобно для изучения и понимания работы кластеров ClickHouse.

2. **Симуляция кластерной среды:**
   - Можно симулировать поведение распределенной системы, что полезно для подготовки к развертыванию в продакшен-среде.

**Недостатки и ограничения:**

1. **Отсутствие реальных преимуществ производительности:**
   - На одном хосте нет реального распределения нагрузки по разным физическим серверам.
   - Все данные хранятся на одном диске и обрабатываются одним набором ресурсов CPU и памяти.

2. **Сложность и дополнительные накладные расходы:**
   - Усложняет конфигурацию системы без явных преимуществ.
   - Может привести к избыточному использованию ресурсов и усложнению администрирования.

3. **Риск ошибок и некорректной работы:**
   - При неправильной настройке могут возникнуть проблемы с согласованностью данных.
   - Возможны конфликты из-за использования одинаковых портов или путей к данным.

**Вывод:**

- **Для учебных и тестовых целей** настройка шардирования на одном хосте может быть оправдана. Это позволяет понять принципы работы распределенных таблиц и отладить приложения перед развертыванием в реальной кластерной среде.

- **Для продакшен-систем** такое решение **не рекомендуется**, поскольку не дает преимуществ в производительности или надежности, а только усложняет систему. Шардирование предназначено для распределения данных и нагрузки по нескольким физическим серверам, обеспечивая масштабируемость и отказоустойчивость.

---

### **Рекомендации**

- **Используйте шардирование на одном хосте только для обучения и тестирования.**

- **Для продакшен-систем**:

  - **Разверните несколько физических или виртуальных серверов** для реального распределения нагрузки.
  - **Настройте репликацию** для повышения отказоустойчивости.
  - **Мониторьте производительность** и корректно распределяйте данные по шардам.

- **Упростите конфигурацию**, если шардирование на одном хосте не дает явных преимуществ для вашего случая использования.

---

### **Дополнительные сведения**

- **Как работает ключ шардирования на одном хосте:**

  - В нашем примере данные распределяются по локальным таблицам на основе значения `user_id`.
  - Несмотря на то, что шарды находятся на одном хосте, ClickHouse выполняет логическое разделение данных.

- **Альтернативный подход без шардирования:**

  - Если у вас один сервер, вы можете использовать обычную таблицу MergeTree без Distributed слоя.
  - Это упростит конфигурацию и повысит производительность за счет отсутствия дополнительных накладных расходов на распределение данных.

---

### **Заключение**

Создание таблиц с шардированием на одном хосте в ClickHouse возможно и может быть полезно для определенных целей, таких как тестирование и обучение. Однако для реальных приложений такое решение не предоставляет преимуществ и может добавить ненужную сложность. Рекомендуется использовать шардирование в тех случаях, когда у вас есть несколько серверов, и вы хотите распределить нагрузку и объем данных для улучшения производительности и масштабируемости.

---

**Ресурсы для дальнейшего изучения:**

- [Официальная документация ClickHouse по кластеризации](https://clickhouse.com/docs/ru/engines/table-engines/special/distributed)
- [Руководство по настройке кластеров](https://clickhouse.com/docs/ru/operations/cluster-setup)
- [Практики шардирования и репликации в ClickHouse](https://clickhouse.com/docs/ru/architecture/capabilities)

---

**Желаю успехов в работе с ClickHouse!**
